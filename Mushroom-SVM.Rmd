---
title: "Mushrooms-SVM"
date: "June 8, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r echo=FALSE}
rm(list = ls())
require(e1071)
require(caret)
require(kernlab)



mushroom <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"), header = FALSE, sep = ",")

set.seed(20)
```
I named the columns and then converted them from factors to numeric as depicted by str() performed before and after transformation. I also dropped veil_type(V17) since it only has one level and may create errors. 
```{r message=FALSE}
str(mushroom)

colnames(mushroom) <- c("edibility", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", "grill_attachment", "grill_spacing", "grill_size", "grill_color", "stalk_shape", "stalk_root", "stalk_surface_above_ring", "stalk_surface_below_ring", "stalk_color_above_ring", "stalk_color_below_ring", "veil_type", "veil_color", "ring_number", "ring_type", "spore_print_color", "population", "habitat")

sum(is.na(mushroom))
mushroom <- subset(mushroom, select = -veil_type)
mushroom$cap_shape <- as.numeric(mushroom$cap_shape)
mushroom$cap_surface <- as.numeric(mushroom$cap_surface)
mushroom$cap_color <- as.numeric(mushroom$cap_color)
mushroom$bruises <- as.numeric(mushroom$bruises)
mushroom$odor <- as.numeric(mushroom$odor)
mushroom$grill_attachment <- as.numeric(mushroom$grill_attachment)
mushroom$grill_spacing <- as.numeric(mushroom$grill_spacing)
mushroom$grill_size <- as.numeric(mushroom$grill_size)
mushroom$grill_color <- as.numeric(mushroom$grill_color)
mushroom$stalk_shape <- as.numeric(mushroom$stalk_shape)
mushroom$stalk_root <- as.numeric(mushroom$stalk_root)
mushroom$stalk_surface_above_ring <- as.numeric(mushroom$cap_shape)
mushroom$stalk_surface_below_ring <- as.numeric(mushroom$cap_shape)
mushroom$stalk_color_above_ring <- as.numeric(mushroom$stalk_color_above_ring)
mushroom$stalk_color_below_ring <- as.numeric(mushroom$stalk_color_below_ring)
mushroom$veil_color <- as.numeric(mushroom$veil_color)
mushroom$ring_number <- as.numeric(mushroom$ring_number)
mushroom$ring_type <- as.numeric(mushroom$ring_type)
mushroom$spore_print_color <- as.numeric(mushroom$spore_print_color)
mushroom$population <- as.numeric(mushroom$population)
mushroom$habitat <- as.numeric(mushroom$habitat)
str(mushroom)
```
We see that 52% of mushrooms are edible and 48% are poisionous. I then split the data 80/20 for train and test respectively.
```{r}
4208/8124
3916/8124
.8 * 8124
.2 * 8124
sam <- sample(8124, 6499)
mush_train <- mushroom[sam, ]
mush_test <- mushroom[-sam, ]
dim(mush_train)
dim(mush_test)
str(mush_train)
```
With the training data I first ran the linear kernel "vanilladot". Which, created a model that is 98% accurate. When variables can be separated perfectly by a straight line or two dimensional flat surface this would be the best fit, and has the speediest output since it really is not a kernel and uses theta transposing. It is said that the linear kernel is best used when number of features is larger than number of observations. If number of observations is larger than 50,000, however speed could be an issue when using the gaussian kernel so then one might want to use the linear kernel.

```{r}

mush_classifier <- ksvm(edibility ~ ., data = mush_train,
kernel = "vanilladot")
print(mush_classifier)
mush_predictions <- predict(mush_classifier, mush_test)
agreement <- mush_predictions == mush_test$edibility
table(agreement)
prop.table(table(agreement))
```
It looks like both the polynomial("polydot") and linear kernels produce the same results at 98% accuracy. However, since the polynomial kernel is seen as computationally and predictively inefficient it is not commonly used. If the Gaussian kernel isn't the best fit we would stick with the linear kernel.
```{r}

mush_classifier_poly <- ksvm(edibility ~ ., data = mush_train,
kernel = "polydot")
print(mush_classifier_poly)
mush_predictions_poly <- predict(mush_classifier_poly, mush_test)
agreement_poly <- mush_predictions_poly == mush_test$edibility
table(agreement_poly)
prop.table(table(agreement_poly))

```
Gaussian is 100%, and overfitting the dataset. Also known as the "radial basis function" kernel it typically will always beat the linear kernel in accuracy. The popular "iris" dataset is an example of one that does best with the linear kernel for the sake of speed, but they tie in accuracy.
```{r}
mush_classifier_rbf <- ksvm(edibility ~ ., data = mush_train,
kernel = "rbf")
print(mush_classifier_rbf)
mush_predictions_rbs <- predict(mush_classifier_rbf, mush_test)
agreement_rbf <- mush_predictions_rbs == mush_test$edibility
table(agreement_rbf)
prop.table(table(agreement_rbf))
```